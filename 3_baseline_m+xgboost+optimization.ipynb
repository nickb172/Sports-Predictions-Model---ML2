{"cells":[{"cell_type":"markdown","metadata":{"id":"ZLZYELILp4o6"},"source":["# **Loading libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EpJDlnGip1HR","outputId":"2f8495fa-c825-4cd4-de3d-97e8d36e934e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n","Requirement already satisfied: cuml-cu12 in /usr/local/lib/python3.12/dist-packages (25.10.0)\n","Requirement already satisfied: cuda-python<13.0a0,>=12.9.2 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (12.9.4)\n","Requirement already satisfied: cuda-toolkit==12.* in /usr/local/lib/python3.12/dist-packages (from cuda-toolkit[cublas,cufft,curand,cusolver,cusparse]==12.*->cuml-cu12) (12.9.1)\n","Requirement already satisfied: cudf-cu12==25.10.* in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (25.10.0)\n","Requirement already satisfied: cupy-cuda12x>=13.6.0 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (13.6.0)\n","Requirement already satisfied: dask-cuda==25.10.* in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (25.10.0)\n","Requirement already satisfied: dask-cudf-cu12==25.10.* in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (25.10.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (1.5.2)\n","Requirement already satisfied: libcuml-cu12==25.10.* in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (25.10.0)\n","Requirement already satisfied: numba-cuda<0.20.0a0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from numba-cuda[cu12]<0.20.0a0,>=0.19.1->cuml-cu12) (0.19.1)\n","Requirement already satisfied: numba<0.62.0a0,>=0.60.0 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (0.60.0)\n","Requirement already satisfied: numpy<3.0a0,>=1.23 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (25.0)\n","Requirement already satisfied: pylibraft-cu12==25.10.* in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (25.10.0)\n","Requirement already satisfied: raft-dask-cu12==25.10.* in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (25.10.0)\n","Requirement already satisfied: rapids-dask-dependency==25.10.* in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (25.10.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (13.9.4)\n","Requirement already satisfied: rmm-cu12==25.10.* in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (25.10.0)\n","Requirement already satisfied: scikit-learn>=1.4 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (1.6.1)\n","Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (1.16.3)\n","Requirement already satisfied: treelite==4.4.1 in /usr/local/lib/python3.12/dist-packages (from cuml-cu12) (4.4.1)\n","Collecting nvidia-cublas-cu12==12.9.1.4.* (from cuda-toolkit[cublas,cufft,curand,cusolver,cusparse]==12.*->cuml-cu12)\n","  Downloading https://pypi.nvidia.com/nvidia-cublas-cu12/nvidia_cublas_cu12-12.9.1.4-py3-none-manylinux_2_27_x86_64.whl (581.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.2/581.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.4.1.4.* (from cuda-toolkit[cublas,cufft,curand,cusolver,cusparse]==12.*->cuml-cu12)\n","  Downloading https://pypi.nvidia.com/nvidia-cufft-cu12/nvidia_cufft_cu12-11.4.1.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.9 MB)\n","\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/200.9 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"]}],"source":["# Install cuml for GPU processing\n","!pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7BjMHOLRE84Y"},"outputs":[],"source":["# Loading all necessary libraries\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.model_selection import TimeSeriesSplit\n","\n","# Preprocessing\n","from sklearn.preprocessing import StandardScaler, FunctionTransformer\n","from sklearn.pipeline import make_pipeline, Pipeline\n","\n","# Sampling\n","from sklearn.kernel_approximation import RBFSampler\n","from sklearn.calibration import CalibratedClassifierCV\n","\n","# Import models\n","from sklearn.dummy import DummyClassifier\n","from cuml.svm import SVC\n","from cuml.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegressionCV, LogisticRegression, PoissonRegressor\n","from sklearn.linear_model import SGDClassifier\n","\n","# Import evaluation metrics\n","from sklearn.metrics import (accuracy_score, precision_score,\n","                            recall_score, f1_score, confusion_matrix,\n","                            classification_report, roc_curve, auc, roc_auc_score)\n","from sklearn.model_selection import cross_validate, cross_val_predict\n","\n","\n","\n","\n","# Random Seed\n","seed = 42"]},{"cell_type":"markdown","metadata":{"id":"RIs9QlbwCteq"},"source":["# **Loading the Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cqkz1OC8D3Rk"},"outputs":[],"source":["# Load dataset\n","url='https://drive.google.com/file/d/1eyqgzDSDuy6n1JZUGodAsiGUTFCzyy2B/view?usp=drive_link'\n","url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n","df = pd.read_csv(url)\n","df.head()\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"490C8iF0C742"},"source":["# **Data Exploration and Preprocessing for Modeling**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XGurXsnMNZvf"},"outputs":[],"source":["# Checking the overall specifications of the data\n","df.info()\n","df_copy=df\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BpEYr0RmlYkt"},"outputs":[],"source":["# Adding month number so we can try and observe if there are months with more goals scored\n","df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n","df['Month'] = df['Date'].dt.month\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cqNWJzauHaz"},"outputs":[],"source":["df.iloc[250]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0_aG1PwqY5o"},"outputs":[],"source":["# Removing initial n number of rows which do not have average values\n","\n","## Creating criteria for rows that are going to be removed from our work\n","\n","rolling_features = [\n","    'avg_goals_scored_home_5', 'avg_goals_conceded_home_5',\n","    'avg_goals_scored_away_5', 'avg_goals_conceded_away_5',\n","    'avg_total_goals_home_5', 'avg_total_goals_away_5',\n","    'win_rate_home_5', 'win_rate_away_5',\n","    'Over2.5_home_5', 'Over2.5_away_5'\n","]\n","\n","\n","# Count how many rolling features are zero or NaN per row\n","zero_or_nan_mask = df[rolling_features].apply(lambda row: ((row == 0) | (row.isna())).sum(), axis=1)\n","\n","# Defining a threshold - NOTE: this is a hard threshold, we could potentially change it to be more flexible based on percentiles\n","threshold = 5\n","\n","\n","\n","df_cleaned = df[zero_or_nan_mask < threshold]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4BpJ-U2uk8f"},"outputs":[],"source":["# Number of rows removed\n","\n","len(df)-len(df_cleaned)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"amY1hDylxEc6"},"outputs":[],"source":["# Checking the distributions of our variables\n","\n","df_cleaned.hist(figsize=(15, 10), bins=30)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"i9BOgRaO3o32"},"source":["As we can see, most of the important features have a normal-ish distribution with some of them being a bit positively skewed (notably the avg_goals variables)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S03usLJs89Cj"},"outputs":[],"source":["# Let us do a square root transformation for the avg_goals variables since they tend to have more of a poisson distribution\n","\n","df_trans = df_cleaned.copy()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwsbpM5u93pb"},"outputs":[],"source":["poisson_features = ['avg_goals_scored_home_5', 'avg_goals_conceded_home_5',\n","    'avg_goals_scored_away_5', 'avg_goals_conceded_away_5',\n","    'avg_total_goals_home_5', 'avg_total_goals_away_5']\n","\n","df_trans[poisson_features] = np.sqrt(df_trans[poisson_features])\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3SXYCLE-RWq"},"outputs":[],"source":["# Checking distributions after transformation\n","df_trans[poisson_features].hist(figsize=(15, 10), bins=30)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_JOM3SX-_MJe"},"source":["These are still not perfect normal distributions, but it is **close enough** so it will not bother our models too much"]},{"cell_type":"markdown","source":["Since our dataset consists of time series data, we will use TimeSeriesSplit for cross-validation in this case.\n","\n","Important to note is that we are using the models on the whole dataset and thus creating General predictive models for all leagues. This approach significantly reduces the computational complexity associated with training and tuning multiple models individually for each nation\n","\n","Furthermore after creation of this general model, it allows much quicker deployment for any new datasets that might be getting released in the future as it is much more generazible and flexible, as it learns patterns across diverse leagues and countries, making it better suited for handling variations in data without requiring extensive retraining.\n","\n","This is ofcourse under the note that we have limited Computation resources and want to limit computational costs."],"metadata":{"id":"YllwYijVc0oB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKFtVoBkqqM8"},"outputs":[],"source":["# Time series split n = 10\n","target = 'Over2.5'\n","X = df_trans.drop(target, axis=1)\n","y = df_trans[target]\n","\n","time_split = TimeSeriesSplit(n_splits=10)\n","\n","\n","\n","for train_index, test_index in time_split.split(X):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYSQlTQ7tYVJ"},"outputs":[],"source":["# Timeseries split on not transformed data (for RandomForest)\n","X1 = df_cleaned.drop(target, axis=1)\n","y1 = df_cleaned[target]\n","\n","time_split = TimeSeriesSplit(n_splits=10)\n","\n","\n","\n","for train_index, test_index in time_split.split(X1):\n","    X1_train, X1_test = X1.iloc[train_index], X1.iloc[test_index]\n","    y1_train, y1_test = y1.iloc[train_index], y1.iloc[test_index]"]},{"cell_type":"markdown","metadata":{"id":"_m5RGDF3Dd9B"},"source":["# **Modeling**\n","Testing out multiple models:\n","\n","* Dummy Classifier\n","* Random Forest\n","* SVM with Stochastic Gradient Descent\n","* SVM (and the Kernels)\n","* Logistic Classification\n","* XGBoost\n"]},{"cell_type":"markdown","source":["The reasons for our choice of machine learning models are as follows:\n","\n","Random forest was chosen as a sort of better than Dummy Classifier basic model to see how far we could get with the power of Random Forests.\n","\n","SGDC due to being ideal for large datasets and it allows us to incorporate regularization techniques to prevent overfitting.\n","\n","SVM RBF and polynomial kernels were chosen because they are strong for capturing complex, non-linear decision boundaries that may come from patterns in match statistics. These kernels allow the model to adapt to subtle variations across different leagues.\n","\n","Logistic regression for its strong baseline for binary classification.\n","\n","And finally XGBoost which just generally excels in handling structured data and capturing feature interactions while maintaining high predictive performance and efficiency. Its regularization capabilities also help mitigate overfitting."],"metadata":{"id":"p09zIV4PcvAU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDkd0Rl0DkTN"},"outputs":[],"source":["# Dummy Classifer\n","dummy = DummyClassifier(strategy='most_frequent')\n","\n","# Random Forest Classifier\n","forest = RandomForestClassifier(n_estimators=500, random_state=seed, max_features='sqrt', max_depth=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aSlc_wiPHvUS"},"outputs":[],"source":["# Linear SVM using SGDC\n","SGDC_pipe = Pipeline([\n","    (\"scaler\", StandardScaler()),\n","    (\"calibrated_sgd\", CalibratedClassifierCV( # To fit SVM to ROC plot, it must be wrapped in calibrater\n","        SGDClassifier(max_iter=1000, tol=1e-3,\n","            random_state=seed,\n","            class_weight='balanced'\n","        ),\n","        cv=5,\n","        method=\"sigmoid\"\n","    ))\n","])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGe3tYwXonvk"},"outputs":[],"source":["# SVM with RBF Kernel\n","rbf_pipe = Pipeline([\n","    (\"scaler\", StandardScaler()),\n","    (\"calibrated_sgd\", CalibratedClassifierCV( # To fit SVM to ROC plot, it must be wrapped in calibrater\n","        SVC(kernel='rbf'),\n","        cv=5,\n","        method=\"sigmoid\"\n","    ))\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgShDl8copef"},"outputs":[],"source":["# SVM with poly kernel\n","poly_pipe = Pipeline([\n","    (\"scaler\", StandardScaler()),\n","    (\"calibrated_sgd\", CalibratedClassifierCV( # To fit SVM to ROC plot, it must be wrapped in calibrater\n","        SVC(kernel='poly'),\n","        cv=5,\n","        method=\"sigmoid\"\n","    ))\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eA6v_8sGos18"},"outputs":[],"source":["# Logistic Regression\n","log_pipe = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('log_reg', LogisticRegression(random_state=seed))\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NBR1fU9izEju"},"outputs":[],"source":["# Dropping Date column so models fit properly\n","X_train = X_train.drop(columns=['Date'])\n","X_test = X_test.drop(columns=['Date'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0rf4fIQnBmOm"},"outputs":[],"source":["X1_train = X1_train.drop(columns=['Date'])\n","X1_test = X1_test.drop(columns=['Date'])"]},{"cell_type":"markdown","metadata":{"id":"WZKR9iEXpLxm"},"source":["# **Evaluation of Models on the Base Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WDDAzSYbqNPx"},"outputs":[],"source":["# Creating an overall evaluation function\n","\n","def evaluate_model(model, X_train, y_train, X_test, y_test):\n","    # Fit the model\n","    model.fit(X_train, y_train)\n","\n","    # Predict on test set\n","    y_pred = model.predict(X_test)\n","\n","    # Compute metrics\n","    accuracy = accuracy_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","\n","    # Print results\n","    print(\"Test Set Performance:\")\n","    print(f\"Accuracy:  {accuracy:.4f}\")\n","    print(f\"F1 Score:  {f1:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall:    {recall:.4f}\")\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-stkEJ30HlVI"},"source":["Creating Helpful AUC model evaluation function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wI3mZFiHV_U"},"outputs":[],"source":["def evaluate_model_AUC(model):\n","    # Get predictions\n","    y_pred_test = model.predict(X_test)\n","    y_pred_train = model.predict(X_train)\n","\n","    # Calculate accuracy\n","    accuracy_test = accuracy_score(y_test, y_pred_test)\n","    accuracy_train = accuracy_score(y_train, y_pred_train)\n","\n","    # Print accuracy\n","    print(f\"Test Accuracy: {accuracy_test}\")\n","    print(f\"Train Accuracy: {accuracy_train}\")\n","\n","    # Plot ROC Curve (if model has predict_proba)\n","    if hasattr(model, 'predict_proba'):\n","        proba_test = model.predict_proba(X_test)\n","        proba_train = model.predict_proba(X_train)\n","\n","        # Safe handling for binary or single-class case\n","        if proba_test.ndim == 2 and proba_test.shape[1] > 1:\n","            y_pred_proba_test = proba_test[:, 1]\n","            y_pred_proba_train = proba_train[:, 1]\n","        else:\n","            # If only one column exists, use that\n","            y_pred_proba_test = proba_test.ravel()\n","            y_pred_proba_train = proba_train.ravel()\n","\n","        # Compute ROC and AUC\n","        fpr, tpr, _ = roc_curve(y_test, y_pred_proba_test)\n","        roc_auc = auc(fpr, tpr)\n","        fpr_train, tpr_train, _ = roc_curve(y_train, y_pred_proba_train)\n","        roc_auc_train = auc(fpr_train, tpr_train)\n","\n","        # Plot ROC curves\n","        plt.figure()\n","        plt.plot(fpr, tpr, label=f'Test ROC curve (area = {roc_auc:.2f})')\n","        plt.plot(fpr_train, tpr_train, label=f'Train ROC curve (area = {roc_auc_train:.2f})')\n","        plt.plot([0, 1], [0, 1], 'k--', label='No Skill')\n","        plt.xlim([0.0, 1.0])\n","        plt.ylim([0.0, 1.05])\n","        plt.xlabel('False Positive Rate')\n","        plt.ylabel('True Positive Rate')\n","        plt.title('ROC Curve')\n","        plt.legend(loc='lower right')\n","        plt.show()\n","    else:\n","        print(\"ROC curve cannot be plotted as the model does not have predict_proba.\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nNn32VFeInPy"},"source":["## Dummy model as a baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGS2u6LFvwP-"},"outputs":[],"source":["# Dummy Performance\n","evaluate_model(dummy, X_train, y_train, X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYjEFJaWH86z"},"outputs":[],"source":["evaluate_model_AUC(dummy)"]},{"cell_type":"markdown","metadata":{"id":"zqIzYyjfIxY1"},"source":["## Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ln9Ubs5vwRiv"},"outputs":[],"source":["# Random Forest Performance (Using X1 and y1 as untransformed for Random Forest)\n","evaluate_model(forest, X1_train, y1_train, X1_test, y1_test)"]},{"cell_type":"markdown","metadata":{"id":"sG6VaqLGI2qq"},"source":["## SVM stochastic gradient descent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxBEChLlwSiS"},"outputs":[],"source":["# SVM SGDC\n","evaluate_model(SGDC_pipe, X_train, y_train, X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7flQjF1lIEA_"},"outputs":[],"source":["evaluate_model_AUC(SGDC_pipe)"]},{"cell_type":"markdown","metadata":{"id":"MG9IwNkLJCJ3"},"source":["## SVM RBF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9V1BIbNCwS17"},"outputs":[],"source":["# SVM RBF\n","evaluate_model(rbf_pipe, X_train, y_train, X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Z2LSd5dIGur"},"outputs":[],"source":["evaluate_model_AUC(rbf_pipe)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KBp25uTYwlyB"},"outputs":[],"source":["# SVM Poly\n","evaluate_model(poly_pipe, X_train, y_train, X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPXBAWxZII2n"},"outputs":[],"source":["evaluate_model_AUC(poly_pipe)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-ex_fGKwTR_"},"outputs":[],"source":["# Logistic Regression\n","evaluate_model(log_pipe, X_train, y_train, X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGB19XvZILpd"},"outputs":[],"source":["evaluate_model_AUC(log_pipe)"]},{"cell_type":"markdown","metadata":{"id":"b8x3egMvXK0n"},"source":["# **2. Optimization and tuning for baseline model**\n"]},{"cell_type":"markdown","metadata":{"id":"8BFd1xUuyxRp"},"source":["For hyperparemeter tuning we will mainly be using BayesSearch because it is much quicker and practical than both GridSearch and RandomSearch, especially when using GPU on colab, due to the limited runtime on GPUs (disconnections after a few minutes with GridSearch). We know it might not guarantee the very best parameters, but we decided it is the most practical in our scenario."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FyEQpnH3BNAz"},"outputs":[],"source":["!pip install scikit-optimize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9ZfoPLpyRjV"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","from skopt import BayesSearchCV\n","from skopt.space import Real, Categorical\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_w7vHpZpzxFI"},"outputs":[],"source":["# SGDC Tuning\n","sgd_search = BayesSearchCV(\n","    estimator=SGDC_pipe,\n","    search_spaces=\n","{\n","    'calibrated_sgd__estimator__alpha': (1e-5, 1e-2, 'log-uniform'),\n","    'calibrated_sgd__estimator__loss': ['hinge', 'log_loss'],\n","    'calibrated_sgd__estimator__penalty': ['l2', 'l1', 'elasticnet']\n","},\n","\n","    n_iter=20,\n","    cv=time_split,\n","    scoring='accuracy',\n","    random_state=seed,\n","    n_jobs=-1\n",")\n","\n","sgd_search.fit(X_train, y_train)\n","\n","# Saving predictions for later evaluation\n","y_pred_sgd = sgd_search.predict(X_test)\n","\n","\n","# --- Show the best results ---\n","print(\"Best parameters found:\", sgd_search.best_params_)\n","print(\"Best accuracy score:\", sgd_search.best_score_)"]},{"cell_type":"markdown","source":["**Tuning SVMs (poly and RBF kernels)** Unfortunately we could not get any reasonable runtimes with the tuning of SVMs, one of our collegues even encountered a usage limit with Google Colabs GPU runtime, where the GPU was disabled for that user after running it for tens of minutes, for an undisclosed amount of time.\n","\n","Due to these constraints, we have not included the tuning results in this report. However, we have left commented code in place to demonstrate the process we had set up for this task."],"metadata":{"id":"Ol2MI2CJYSX6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGo0M6kT2DXb"},"outputs":[],"source":["# Tuning SVMs\n","\n","# Creating new pipeline so it fits into the bayes search and tests if either RBF or poly is better for accuracy overall\n","# svm_pipe = Pipeline([\n","#    (\"scaler\", StandardScaler()),\n","#    (\"calibrated_svm\", CalibratedClassifierCV(\n","#        estimator=SVC(random_state=seed),\n","#        cv=time_split,\n","#        method=\"sigmoid\"\n","#    ))\n","#])\n","\n","# Bayesian Search\n","#svm_bayes_search = BayesSearchCV(\n","#    estimator=svm_pipe,\n","#    search_spaces= {\n","#    'calibrated_svm__estimator__C': (0.1, 10.0, 'log-uniform'),\n","#    'calibrated_svm__estimator__gamma': (1e-3, 1.0, 'log-uniform'),\n","#    'calibrated_svm__estimator__kernel': ['rbf', 'poly'],\n","#    'calibrated_svm__estimator__degree': (2, 5)  # only relevant if kernel='poly'\n","#},\n","#    n_iter=20,\n","#    cv=time_split,\n","#    scoring='accuracy',\n","#    random_state=42,\n","#    n_jobs=-1\n","#)\n","\n","# Fit\n","#svm_bayes_search.fit(X_train, y_train)\n","\n","# Best params and score\n","#print(\"Best Parameters:\", svm_bayes_search.best_params_)\n","#print(\"Best accuracy score:\", svm_bayes_search.best_score_)"]},{"cell_type":"markdown","source":["For Random Forest, we had to resort to using GridSearch, as BayesSearch did not work properly in this case and produced unexpected errors.\n","\n","Fortunately, the GridSearch run completes relatively quickly, so we decided to stick with this approach for tuning this model."],"metadata":{"id":"5Q7ixzYKa15y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I25xvZ2I2FWk"},"outputs":[],"source":["# Tuning Random Forest\n","from sklearn.metrics import accuracy_score, make_scorer\n","from sklearn.model_selection import GridSearchCV\n","accuracy_scorer = make_scorer(accuracy_score)\n","\n","\n","param_grid_forest = {\n","    'n_estimators': [100, 200, 300],\n","    'max_features': ['sqrt', 'log2'],\n","    'max_depth': [5, 10, 15, 20]\n","}\n","\n","grid_forest = GridSearchCV(forest, param_grid=param_grid_forest, cv=time_split, scoring='accuracy', n_jobs=-1) # Use original X and y, it will be processed by time_series_cv()\n","grid_forest.fit(X_train,y_train)\n","\n","# Best params and score\n","print(\"Best Parameters:\", grid_forest.best_params_)\n","print(\"Best accuracy score:\", grid_forest.best_score_)"]},{"cell_type":"code","source":["best_forest = RandomForestClassifier(random_state = seed, max_depth = 5, max_features= 'sqrt', n_estimators= 100)\n","\n","best_forest.fit(X_train, y_train)\n","y_pred_forest = best_forest.predict(X_test)\n","accuracy_score(y_test, y_pred_forest)"],"metadata":{"id":"NxGJJwSwKJej"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6T816CmG-yUW"},"source":["In the next cell, we will tune logistic regression model and use Bayesian optimization to find the best regularization parameter and solver."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4da05fde"},"outputs":[],"source":["# --- Define pipeline ---\n","log_pipe = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('log_reg', LogisticRegression(random_state=seed, max_iter=200))\n","])\n","\n","# --- Define parameter space ---\n","param_space = {\n","    'log_reg__C': Real(0.001, 10, prior='log-uniform'),\n","    'log_reg__penalty': Categorical(['l1', 'l2']),\n","    'log_reg__solver': Categorical(['liblinear', 'saga'])\n","}\n","\n","\n","# --- Define the Bayesian optimizer ---\n","bayes_search = BayesSearchCV(\n","    estimator=log_pipe,\n","    search_spaces=param_space,\n","    n_iter=20,                  # number of iterations (budget)\n","    scoring='f1',\n","    cv=time_split,\n","    n_jobs=-1,\n","    random_state=seed,\n","    verbose=1\n",")\n","\n","# --- Fit the optimizer ---\n","bayes_search.fit(X_train, y_train)\n","\n","# --- Show the best results ---\n","print(\"Best parameters found:\", bayes_search.best_params_)\n","print(\"Best F1 score:\", bayes_search.best_score_)\n","\n","# --- Evaluate on test set ---\n","optimized_model = bayes_search.best_estimator_\n","y_pred = optimized_model.predict(X_test)\n","\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4b3457eb"},"outputs":[],"source":["lr_optimalized_model = Pipeline([\n","    (\"scaler\", StandardScaler()),\n","    (\"log_reg\", LogisticRegression(\n","        C=bayes_search.best_params_[\"log_reg__C\"],\n","        penalty=bayes_search.best_params_[\"log_reg__penalty\"],\n","        solver=bayes_search.best_params_[\"log_reg__solver\"], # Use the best solver\n","        # l1_ratio=bayes_search.best_params_.get(\"log_reg__l1_ratio\"), # Remove l1_ratio as it's not needed with liblinear or l1\n","        max_iter=500,\n","        random_state=seed\n","    ))\n","])\n","\n","lr_optimalized_model.fit(X_train, y_train)\n","y_pred = lr_optimalized_model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c23cf08a"},"outputs":[],"source":["evaluate_model_AUC(lr_optimalized_model)"]},{"cell_type":"markdown","metadata":{"id":"P79ASeboO5sC"},"source":["Bayesian optimization unfortunately did not help in this case."]},{"cell_type":"markdown","metadata":{"id":"ZkFp0RtnaaJf"},"source":["## XGBoost\n","\n","---\n","Let us try XGBoost to see if it performs better than the previous models.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERlM1390XJWw"},"outputs":[],"source":["#pip install xgboost optuna\n","import xgboost as xgb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"re1EhqFqET6E"},"outputs":[],"source":["# duplicate test and train sets\n","y_train_xgb = y_train\n","y_test_xgb = y_test\n","X_train_xgb = X_train\n","X_test_xgb = X_test\n"]},{"cell_type":"markdown","metadata":{"id":"8cRzOmh-bGJh"},"source":["XGB classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YV_jf7HvEVHC"},"outputs":[],"source":["xgb_model = xgb.XGBClassifier(\n","    n_estimators=100,\n","    max_depth=3,\n","    learning_rate=0.05,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    random_state=seed,\n","    use_label_encoder=False,\n","    eval_metric='logloss'\n",")\n","\n","xgb_model.fit(X_train_xgb, y_train_xgb)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-C025IXEl_r"},"outputs":[],"source":["evaluate_model_AUC(xgb_model)"]},{"cell_type":"markdown","metadata":{"id":"JFaNlI8oHdjj"},"source":["XGBoost performs slighty better, but overfits. Let us optimize parameters"]},{"cell_type":"markdown","metadata":{"id":"rJpcFkvWwRc1"},"source":["### Optimize hyperparameters of XGBoost"]},{"cell_type":"markdown","metadata":{"id":"3aJtidHhweWE"},"source":["Now I´ll try to get better porformance by aplying optuna to find best parameters for the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRJDJilLv8sI"},"outputs":[],"source":["!pip install bayesian-optimization\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gTNpa3Sjw6nQ"},"outputs":[],"source":["from xgboost import XGBClassifier\n","from sklearn.model_selection import cross_val_score\n","from bayes_opt import BayesianOptimization\n","import numpy as np\n","\n","# Definujeme hledanou funkci\n","def xgb_evaluate(max_depth, learning_rate, subsample, colsample_bytree, gamma):\n","    # Převod float -> int pro max_depth\n","    max_depth = int(max_depth)\n","\n","    model = XGBClassifier(\n","        n_estimators=100,\n","        max_depth=max_depth,\n","        learning_rate=learning_rate,\n","        subsample=subsample,\n","        colsample_bytree=colsample_bytree,\n","        gamma=gamma,\n","        objective='binary:logistic',\n","        eval_metric='logloss',\n","        use_label_encoder=False,\n","        random_state=seed\n","    )\n","\n","    # 5-fold cross-validation\n","    score = cross_val_score(model, X_train_xgb, y_train_xgb, cv=5, scoring='roc_auc').mean()\n","    return score\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjY33YqIw8tY"},"outputs":[],"source":["# Ranges for optimization\n","pbounds = {\n","    'max_depth': (3, 10),\n","    'learning_rate': (0.01, 0.3),\n","    'subsample': (0.5, 1.0),\n","    'colsample_bytree': (0.5, 1.0),\n","    'gamma': (0, 5)\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ACeKi6PwxGE2"},"outputs":[],"source":["optimizer = BayesianOptimization(\n","    f=xgb_evaluate,\n","    pbounds=pbounds,\n","    random_state=seed,\n","    verbose=2\n",")\n","\n","optimizer.maximize(\n","    init_points=5,\n","    n_iter=25\n",")\n","\n","print(optimizer.max)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8WerUOuwDQI"},"outputs":[],"source":["best_params = optimizer.max['params']\n","final_model = XGBClassifier(\n","    n_estimators=100,\n","    max_depth=int(best_params['max_depth']),\n","    learning_rate=best_params['learning_rate'],\n","    subsample=best_params['subsample'],\n","    colsample_bytree=best_params['colsample_bytree'],\n","    gamma=best_params['gamma'],\n","    objective='binary:logistic',\n","    eval_metric='logloss',\n","    use_label_encoder=False,\n","    random_state=seed\n",")\n","\n","final_model.fit(X_train_xgb, y_train_xgb)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LEeoclV4xiNy"},"outputs":[],"source":["evaluate_model_AUC(final_model)"]},{"cell_type":"markdown","metadata":{"id":"qA1SQ1oDyoTc"},"source":["We added a validation set, applied early stop and try it optuna again with different settings.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ir-de_Lc4OWA"},"outputs":[],"source":["!pip install optuna\n","import xgboost as xgb\n","import optuna\n","from sklearn.metrics import log_loss, accuracy_score, roc_curve, auc\n","import matplotlib.pyplot as plt\n","\n","# Split train into train/eval for Optuna tuning\n","cut = int(len(X_train_xgb) * 0.85)\n","X_tr, X_eval = X_train_xgb.iloc[:cut], X_train_xgb.iloc[cut:]\n","y_tr, y_eval = y_train_xgb.iloc[:cut], y_train_xgb.iloc[cut:]\n","\n","def objective(trial):\n","    params = {\n","        'objective': 'binary:logistic',\n","        'eval_metric': 'logloss',\n","        'tree_method': 'hist',\n","        'seed': seed,\n","        'random_state': seed,\n","        'eta': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n","        'max_depth': trial.suggest_int('max_depth', 2, 6),\n","        'min_child_weight': trial.suggest_int('min_child_weight', 2, 20),\n","        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n","        'gamma': trial.suggest_float('gamma', 0.0, 2.0),\n","        'lambda': trial.suggest_float('reg_lambda', 1.0, 50.0, log=True),\n","        'alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),\n","        'verbosity': 0,\n","    }\n","\n","    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n","    deval = xgb.DMatrix(X_eval, label=y_eval)\n","\n","    num_boost_round = 3000\n","    booster = xgb.train(\n","        params,\n","        dtrain,\n","        num_boost_round=num_boost_round,\n","        evals=[(deval, 'eval')],\n","        early_stopping_rounds=100,\n","        verbose_eval=False\n","    )\n","\n","    best_rounds = getattr(booster, 'best_iteration', None)\n","    if best_rounds is None:\n","        best_rounds = getattr(booster, 'best_ntree_limit', num_boost_round)\n","\n","    trial.set_user_attr('best_rounds', int(best_rounds))\n","\n","    try:\n","        proba = booster.predict(deval, iteration_range=(0, int(best_rounds)))\n","    except TypeError:\n","        proba = booster.predict(deval, ntree_limit=int(best_rounds))\n","\n","    return float(log_loss(y_eval, proba))\n","\n","# Run Optuna study\n","study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=seed))\n","study.optimize(objective, n_trials=40, show_progress_bar=True)\n","\n","best_params = study.best_params\n","best_rounds = int(study.best_trial.user_attrs.get('best_rounds', 300))\n","print(\"Best params:\", best_params)\n","print(\"Best eval logloss:\", study.best_value)\n","print(\"Best boosting rounds:\", best_rounds)\n","\n","# Train final model on full training set\n","final_params = {\n","    'objective': 'binary:logistic',\n","    'eval_metric': 'logloss',\n","    'tree_method': 'hist',\n","    'eta': best_params['learning_rate'],\n","    'max_depth': best_params['max_depth'],\n","    'min_child_weight': best_params['min_child_weight'],\n","    'subsample': best_params['subsample'],\n","    'colsample_bytree': best_params['colsample_bytree'],\n","    'gamma': best_params['gamma'],\n","    'lambda': best_params['reg_lambda'],\n","    'alpha': best_params['reg_alpha'],\n","    'verbosity': 0,\n","}\n","\n","dfull = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n","final_booster = xgb.train(\n","    final_params,\n","    dfull,\n","    num_boost_round=best_rounds,\n","    verbose_eval=False\n",")\n","\n","# Predictions\n","dtrain = xgb.DMatrix(X_train_xgb)\n","dtest = xgb.DMatrix(X_test_xgb)\n","\n","p_train = final_booster.predict(dtrain)\n","p_test = final_booster.predict(dtest)\n","\n","y_pred_train = (p_train >= 0.5).astype(int)\n","y_pred_test = (p_test >= 0.5).astype(int)\n","\n","# Accuracy\n","acc_train = accuracy_score(y_train_xgb, y_pred_train)\n","acc_test = accuracy_score(y_test_xgb, y_pred_test)\n","\n","print(f\"Train Accuracy: {acc_train:.3f}\")\n","print(f\"Test  Accuracy: {acc_test:.3f}\")\n","\n","# ROC curves + AUC\n","fpr_tr, tpr_tr, _ = roc_curve(y_train_xgb, p_train)\n","fpr_te, tpr_te, _ = roc_curve(y_test_xgb, p_test)\n","auc_tr = auc(fpr_tr, tpr_tr)\n","auc_te = auc(fpr_te, tpr_te)\n","\n","plt.figure()\n","plt.plot(fpr_te, tpr_te, label=f\"Test ROC (AUC={auc_te:.2f})\")\n","plt.plot(fpr_tr, tpr_tr, label=f\"Train ROC (AUC={auc_tr:.2f})\", alpha=0.6)\n","plt.plot([0, 1], [0, 1], \"k--\", label=\"No Skill\")\n","plt.xlim([0, 1]); plt.ylim([0, 1.05])\n","plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n","plt.title(\"ROC Curve — XGBoost Booster\")\n","plt.legend(loc=\"lower right\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"yRaNB2tV6Egn"},"source":["This overfits even more and the accuracy for test set does not rise."]},{"cell_type":"markdown","metadata":{"id":"05stP6Fk7Dt2"},"source":["#Best baseline model"]},{"cell_type":"markdown","metadata":{"id":"u0oF4vmp9HXM"},"source":["After applying various tuning and boosting methods, the best baseline model is SVM stochastic gradient descent with a specific set of hyperparameters with the accuracy of roughly 0.545."]},{"cell_type":"markdown","metadata":{"id":"KlNfm_KKF-uL"},"source":["# Performance of the best optimized model for each Country"]},{"cell_type":"markdown","metadata":{"id":"qNNW7uQ3KMF5"},"source":["**Fully Optimized SGDC**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YrxFgMaDGinL"},"outputs":[],"source":["new_df = pd.DataFrame({\n","    'y_test': y_test,\n","    'y_pred_test': y_pred_sgd,\n","    'Div_enc': X_test['Div_enc'],\n","})\n","\n","# Mapping\n","country_to_code = {\n","    'Belgium': [0],\n","    'Germany': [1, 2],\n","    'England': [3, 4, 5, 6],\n","    'France': [7, 8],\n","    'Greece': [9],\n","    'Italy': [10, 11],\n","    'Netherland': [12],\n","    'Portugal': [13],\n","    'Scotland': [14, 15, 16, 17],\n","    'Spain': [18, 19],\n","    'Turkey': [20]\n","}\n","\n","\n","# De-Encode\n","code_to_country = {code: country for country, codes in country_to_code.items() for code in codes}\n","\n","# Convert your numeric series\n","new_df[\"Country\"] = new_df[\"Div_enc\"].map(code_to_country)\n","print(new_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UWdo5oewH0DE"},"outputs":[],"source":["def accuracy(group):\n","    return pd.Series({'accuracy': accuracy_score(group['y_test'], group['y_pred_test'])})\n","\n","accuracies = new_df.groupby('Country').apply(accuracy, include_groups=False)\n","accuracy_report = accuracies.sort_values(by='accuracy', ascending=False)\n","accuracy_report"]},{"cell_type":"markdown","source":["As we can see generally the worst performing Nation for this basic data model is Spain and the best is Portugal. With this in mind we can follow through with the Alternative model with the extended dataset and further compare and analyze the final results."],"metadata":{"id":"qNDrY9zQjfTC"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}